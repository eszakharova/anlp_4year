{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Корпус\n",
    "Корпус был собран с помощью запросов к НКРЯ. \n",
    "#### Глава1 - head.\n",
    "+ глава      \n",
    "    на расстоянии от -7 до 7 от район|отдел|муниципальный|районный|управление|сказать|говорить|заместитель|сообщать|сообщить|слово  \n",
    "    \n",
    "#### Глава2 - chapter\n",
    "+ глава  \n",
    "    на расстоянии от -7 до 7 от книга|читать|интересный|писать|повествовать|роман|произведение  \n",
    "\n",
    "**Из выдачи по обоим запросам удаляются примеры, содержащие \"во главе\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preprocessing import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('head.txt') as file:\n",
    "    head_lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('chapter.txt') as file:\n",
    "    chapter_lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(head_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "head_sentences = []\n",
    "chapter_sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, len(head_lines), 2):\n",
    "    sentence = re.sub('\\[.*?\\]', ' ', head_lines[i])\n",
    "    sentence = sentence.replace('←…→', '')\n",
    "    sentence = sentence.lower()\n",
    "    if 'во главе' not in sentence:\n",
    "        head_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, len(chapter_lines), 2):\n",
    "    sentence = re.sub('\\[.*?\\]', ' ', chapter_lines[i])\n",
    "    sentence = sentence.replace('←…→', '')\n",
    "    sentence = sentence.lower()\n",
    "    if 'во главе' not in sentence:\n",
    "        chapter_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(head_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chapter_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Препроцессинг \n",
    "+ токенизация\n",
    "+ лемматизация\n",
    "+ удаление стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "head_tokens = [tokenize(sent) for sent in head_sentences]\n",
    "chapter_tokens = [tokenize(sent) for sent in chapter_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['становиться',\n",
       " 'писать',\n",
       " 'глава',\n",
       " 'который',\n",
       " 'я',\n",
       " 'прочитывать',\n",
       " 'очень',\n",
       " 'я',\n",
       " 'трогать']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapter_tokens[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# окно - 4 слова слева и справа от целевого\n",
    "# head - 0\n",
    "# chapter - 1\n",
    "entries_with_classes = []\n",
    "for i, corpus in enumerate([head_tokens, chapter_tokens]):\n",
    "    for token_set in corpus:\n",
    "        token_set_copy = list(token_set)\n",
    "        try:\n",
    "            ind = token_set_copy.index('глава')\n",
    "        except ValueError:\n",
    "            ind = token_set_copy.index('гл')\n",
    "        if ind - 4 < 0:\n",
    "            start = 0\n",
    "        else:\n",
    "            start = ind-4\n",
    "        if ind + 5 > len(token_set_copy):\n",
    "            finish = len(token_set_copy)\n",
    "        else:\n",
    "            finish = ind + 5  \n",
    "        token_set_copy = token_set_copy[start:finish]\n",
    "        try:\n",
    "            token_set_copy.remove('глава')\n",
    "        except ValueError:\n",
    "            token_set_copy.remove('гл')\n",
    "        entries_with_classes.append((' '.join(token_set_copy), i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Признаки - count vectorizer и tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = [entry[0] for entry in entries_with_classes]\n",
    "y = [entry[1] for entry in entries_with_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_vect = TfidfVectorizer(analyzer='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_classifier(clf, metric='f1'):\n",
    "    scores = cross_val_score(pipeline, np.asarray(X), np.asarray(y), cv=5, scoring=metric)\n",
    "    score = sum(scores) / len(scores)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb =  MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 for Naive Bayes with CountVectorizer on cross-validation: 0.921436612854\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vectorizer', count_vect),\n",
    "    ('classifier', nb)])\n",
    "print ('F1 for Naive Bayes with CountVectorizer on cross-validation:', score_classifier(pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 for RandomForest with CountVectorizer on cross-validation: 0.88600171484\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vectorizer', count_vect),\n",
    "    ('classifier', rf)])\n",
    "print ('F1 for RandomForest with CountVectorizer on cross-validation:', score_classifier(pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 for Naive Bayes with TfidfVectorizer on cross-validation: 0.918010355657\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vectorizer', tf_vect),\n",
    "    ('classifier', nb)])\n",
    "print ('F1 for Naive Bayes with TfidfVectorizer on cross-validation:', score_classifier(pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 for RandomForest with TfidfVectorizer on cross-validation: 0.874339847603\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vectorizer', tf_vect),\n",
    "    ('classifier', rf)])\n",
    "print ('F1 for RandomForest with TfidfVectorizer on cross-validation:', score_classifier(pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_vect = count_vect.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_vect, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = count_vect.get_feature_names()\n",
    "feature_importances = rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp = zip(feature_names, feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp = sorted(imp, key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('книга', 0.15531678781661135),\n",
       " ('заместитель', 0.045383975057919808),\n",
       " ('район', 0.036928736915023137),\n",
       " ('читать', 0.033897454222876523),\n",
       " ('роман', 0.032149113463298927),\n",
       " ('писать', 0.029477321997815543),\n",
       " ('говорить', 0.028512194686581505),\n",
       " ('администрация', 0.025516321338822485),\n",
       " ('сообщать', 0.02521180757394521),\n",
       " ('первый', 0.0126189781396976)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наиболее важные признаки, полученные с помощью Random Forest:\n",
    "  Наличие в контексте слов:\n",
    "+ ('книга', 0.15531678781661135), - chapter\n",
    "+ ('заместитель', 0.045383975057919808) - head\n",
    "+ ('район', 0.036928736915023137) - head\n",
    "+ ('читать', 0.033897454222876523) - chapter\n",
    "+ ('роман', 0.032149113463298927) - chapter\n",
    "+ ('писать', 0.029477321997815543) - chapter\n",
    "+ ('говорить', 0.028512194686581505) - head\n",
    "+ ('администрация', 0.025516321338822485) - head\n",
    "+ ('сообщать', 0.02521180757394521) - head\n",
    "+ ('первый', 0.0126189781396976) - chapter\n",
    "Большиство слов совпадает с теми, которые были использованы для сбора корпуса. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
